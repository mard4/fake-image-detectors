\documentclass[conference]{IEEEtran} % Usa la classe IEEEtran per formattazione simile

\usepackage{amsmath}
\usepackage{graphicx}



\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

\title{Exploring the Adversarial Robustness of AI-generated Image Detectors}

\author{
    \IEEEauthorblockN{Thomas Lazzerini, Samuele Cappelletti, Martina D'Angelo}
    \IEEEauthorblockA{
        University of Trento
    }
}

\maketitle

\begin{abstract}
    TODO
\end{abstract}

\section{Introduction}
    Synthetic images are now flooding the real world. From online dating sites to social media, fake profiles and scams are everywhere. The problem with synthetic images is that, while some of them are funny and harmless, others could be harmful, they could be exploited by malicious users \cite{carlini2020evading}. In relation to this, in the image forensic field there is a continuous fight between \textit{fake image detectors} and \textit{adversarial attacks}. On one hand, the detectors try to distinguish fake images from real ones, while, on the other hand, the attacks try to trick the detectors by manipulating the images (both real and fake ones). In order to detect fake images, we can exploit the traces/artifacts that fake image generators leave on the generated images. To do so, we have two main types of techniques: the \textit{low-level forensic techniques} and the \textit{high-level forensic techniques}. To former focuses on the pixel-level artifacts, which are almost invisible to the human eye. The latter focuses on physical inconsistencies and on repeated and uniform patterns, both of which are mostly visible to the human eye. Examples of physical inconsistencies are lighting, shadows, reflections or vanishing points inconsistencies \cite{farid2022lighting}\cite{farid2022perspective}. While, an example of repeated and uniform patterns, typical of GAN-based image generators, is the generation of the mouth, the nose and the eyes always in the same position \cite{mundra2023exposing}. In general, we prefer to rely on low-level artifacts since fake image generators are becoming smarter every day, thus they are learning to generate always more realistic images, with fewer physical inconsistencies.

\section{Detectors}

    In this section we will briefly describe a couple of fake image detectors: one uses CLIP to extract the feature vectors from the images \cite{cozzolino2024raising} and one identifies the low-level traces/artifacts by training a GAN and a Diffusion Model \cite{corvi2023detection}.

    \subsection{CLIP-Based Detector}
        Many SoTA fake image detectors works very well in detecting fake images that are generated by an image generator of the same family of the generator that generated the images that they were trained on. But, the problem is that their performance decreases a lot when trying to detect images generated by another type of detector. For example, if the images used to train the detector were generated by a GAN-based generator, then the detector is good in detecting images generated by other GAN-based generators but, it is bad in detecting images generated by a Diffusion-based generator. Moreover, the SoTA detectors hardly generalize to new and unseen generative methods.

        On the other hand, the CLIP-based detector proposed by \cite{cozzolino2024raising} works well in detecting images generated by any type of generator, both with and without augmentations (e.g., cropping, resizing, compression, etc.). Moreover, the performance of this CLIP-based method is similar to the one of the SoTA detectors in the in-distribution scenario but, it has a significant improvement in the out-of-distribution scenario. This is thanks to the fact that the CLIP features achieve an excellent generalization and robustness even with a few examples (e.g., 1k or 10k).

        The CLIP-based method consists in: collect $N$ real images $\{R_1,\dots,R_N\}$ with their corresponding captions $\{t_1,\dots,t_N\}$. Then, use these $N$ captions to generate N images $\{F_1,\dots,F_N\}$ using some image generator $G(.)$, $F_i = G(t_i)$. Successively, use CLIP to extract the feature vectors $\{\mathbf{r}_1,\dots,\mathbf{r}_N\}$ and $\{\mathbf{f}_1,\dots,\mathbf{f}_N\}$ of the $N + N$ images (real + generated), $\mathbf{r}_i = CLIP(R_i)$ and $\mathbf{f}_i = CLIP(F_i)$. Finally, feed the $N + N$ feature vector to a linear SVM classifier.
        
        In the paper they compared the results (in AUC) of the CLIP-based method with other SoTA detectors on images generated by different generators of different families: \textit{GAN}, \textit{Diffusion} and \textit{Commercial Tools}, with and without post-processing on the images. In the latter, the performance of the CLIP-based method is the best one in average wrt. the performance of the other SoTA methods tested. In the case of post-processed images the results are slightly worse but, especially for the CLIP-based method, they are still good across all generators.

    \subsection{Detection of Images by Diffusion Models} 
        Lately, \textit{Diffusion Models} gained the spotlight in the image generation community, allowing for unmatched test-to-image photorealism and diversity. These new powerful tools are a new asset in the hands of malicious users, posing new challenges to the forensic community. Most SoTA detectors exploits low-level artifacts, not visible by a human eye, introduced during the generation phase by GAN generators. The study in \cite{corvi2023detection} suggests that, as can be seen in Fig. \ref{fig:pizza_fourier}, similar traces can be found also in DM-generated images

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\linewidth]{Img/pizza_fourier.png}
            \caption{Fourier transform of of the fingerprint of some DM architectures (\textit{GLIDE} \cite{nichol2021glide}, \textit{Latent Diffusion} \cite{rombach2022high}, \textit{Stable Diffusion} \cite{stablediffusion2022}, \textit{ADM} \cite{dhariwal2021diffusion}, \textit{DALL$\cdot$E 2} \cite{ramesh2022hierarchical}) presented in \cite{corvi2023detection}}
            \label{fig:pizza_fourier}
        \end{figure}

        The study in \cite{corvi2023detection} also provides interesting evaluation results, comparing the performances of several SoTA detectors over different GAN and DM generators both in ideal case (uncompressed images) and real case (compressed and resized using the guidelines in \cite{vipcuplink}). These evaluations highlight how performances vary significantly between the models, due to the differences in their artifacts, therefore suggesting generalization difficulties (for example, in classifying a DM images with a GAN training and vice versa). Despite these difficulties, the inclusion of DM during training and performing an careful calibration procedure, like the one suggested by \cite{Platt1999probabilistic}, may help the generalization over similar architectures, despite not providing reliable results on out-of-training artifacts.

\section{Attacks}
    Despite the powerful detectors at our disposal, there exists many users that aim at attacking such detectors, in order to hide traces of their forgeries or also to introduce traces typical of generated images, to hide disguise content as fake. In the following chapter, some newly developed attacking techniques are discussed, to provide a general overview of the attacker-side.

    \subsection{Mimicry attack against image splicing forensic}
        As stated in \cite{boato2024adversarial}, this \textit{mimicry adversarial attack} can be used to hide image manipulation while forcing the detector to detect arbitrary ones by applying a gradient based optimization approach. Applied at large scale, this would cause high false-alarms, producing an effect similar to \textit{DoS} attacks while undermining the reliability of the target detector.

        The attack strategy proposed in \cite{boato2024adversarial} involves splitting the image in uniform patches and use these to compute a target representation for both the \textit{pristine patch $t_p$}, computed from the pristine patches, and the \textit{forged patch $t_f$}, computed from the forged patches. The function used for computing such target representations needs to be defined for each detector for the attack to be effective, this due to the fact that different detectors exploit different features. Once the targets have been computed, a gradient-based iterative approach is applied the each patch of the manipulated image, in order to make the patch feature representation more similar to the respective target's feature representation. A visualization of such iterative approach can be seen in Fig. \ref{fig:mim_visual}

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\linewidth]{Img/mim_visual.png}
            \caption{Visualization of the mimicry attack strategy and its effects proposed in \cite{boato2024adversarial}. The "\textit{ground truth}" tampering map represents the real forgery, while the "\textit{target}" tampering map represent the arbitrary forgery the attacker wants the detector to output}
            \label{fig:mim_visual}
        \end{figure}

        The evaluation results reported in \cite{boato2024adversarial} suggests this attack is highly effective, both in hiding the real forgery and also highlighting a "decoy" forgery arbitrarily introduced. Two image detectors were tested, \textit{Noiseprint} \cite{cozzolino2019noiseprint} and \textit{EXIF-SC} \cite{huh2018fighting}, over two different datasets, \textit{Columbia} \cite{ng2004data} and \textit{DSO-1} \cite{carvalho2015illuminant}. Several threshold-based and threshold-less metrics have been tested, with the latter being more important from the attacker point of view since the threshold values are unknown to him.

        Another interesting result presented in \cite{boato2024adversarial} regards the \textit{cross-detector} scenario, in which the attack is performed targeting a specific detector but then another is used in the evaluation. Also, \textit{stacked attacks} are considered, in which an image is sequentially attacked against different detectors. An evaluation in these regards reveal mixed results: a misaligned attack in not effective, while the performances of a stacked attack are highly dependant on both the order of the attacks and the detector used in the evaluation. Nevertheless, this is an interesting scenario open for further studies.

    \subsection{Image Laundering with Stable Diffusion}
        \label{sec:laundering}
        Differently from "classic" diffusion models, like \textit{Latent Diffusion}, \textit{Stable Diffusion} models allow the users to provide an initial image as input \cite{sd1_github} \cite{sd2_github} \cite{podell2023sdxl} \cite{sauer2023adversarial-sdxl-turbo}. This image will be superimposed with noise and modified by the model according to the textual prompt. The weight of such modifications can be set via a dedicated strength parameter in the range $[0, 1]$.

        Processing images in such pipeline using a strength parameter equal to 0 produces outputs with the maximum similarity to the inputs: the image is encoded and decoded right away, without any denoising step. As suggested in \cite{mandelli2024synthetic}, this process could be exploited by malicious users in order to mask real content as synthetic. In fact, the encoding/decoding is sufficient to introduce enough artifacts into the real image to make it synthetic in the eyes of numerous detectors. This practice is known as \textit{image laundering}.

        The study in \cite{mandelli2024synthetic}, proposes a two-step architecture, visualized in Fig. \ref{fig:launder_2_stage}, as solution to efficiently discriminate between real, fully synthetic and laundered images. Such architecture is inspired by \cite{mandelli2022detecting}, in which the image is split into multiple random patches, a score is assigned to each one and the average o the highest scores provide the global score of the image: a positive score suggest a synthetic image, while a negative score a real one. Despite the good results, this backbone architecture alone is unfit for the laundered image detection task, hence the introduction of the 2 steps: the first step discriminate real from synthetic images, while the second step discriminate fully synthetic from laundered images.

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.95\linewidth]{Img/launder_2_stage.png}
            \caption{Visualization of the 2 step architecture proposed in \cite{mandelli2024synthetic} for the laundered image classification task. The first step discriminate real images from synthetic (both fully and laundered) ones, while the second step discriminate fully synthetic images from laundered ones}
            \label{fig:launder_2_stage}
        \end{figure}

        The evaluation provided in \cite{mandelli2022detecting} about such 2 step pipeline are good: the first step reach a good separability between real and synthetic image, while the second step reach almost perfect results over multiple models and multiple metrics, with only minor decreases in performances when post-processing operations like JPEG compression and resize are applied.

    \subsection{White Black}
        Evaluating deepfake-image detectors reveals their vulnerabilities to both white- and black-box attacks, significantly reducing their effectiveness. For instance, powerful forensic classifiers can be compromised to achieve near 0\% accuracy under various attack scenarios. A state-of-the-art classifier, as demonstrated by Wang et al. \cite{Wang_2020_CVPR}, achieves an area under the ROC curve (AUC) of 0.95 when trained on a single generator, yet remains susceptible to adversarial perturbations \cite{mardeenpaper1}.
        The attacks are categorized into two conditions:
        \begin{itemize}
            \item White-box attacks, where full access to the classifier's parameters is available.
            \item Black-box attacks, where only the classifier type is known, utilizing adversarial examples that transfer misclassifications across models
        \end{itemize}
        
        They introduce four attack types targeting the classifier from Wang et al. \cite{Wang_2020_CVPR}:
        \begin{itemize}
            \item Image-specific attacks: These modify input images with perturbations to deceive detectors (eg. PGD, DI2-FGSM)
            \begin{itemize}
            \item Loss-Maximizing Attack: this attack maximizes the likelihood that a fake image $x$ perturbed by $\delta$ is misclassified as real. This attack was found to be highly effective and even with flipping the lowest-order bit of 40\% of pixels for uncompressed images, the AUC reduces from 0.966 to 0.27.
            \end{itemize}
            \item Universal Attacks: These create a single adversarial perturbation applicable across various images, reducing computational costs.
            \begin{itemize}
                \item  Universal Adversarial-Patch Attack: A universal patch overlaid on images reduces AUC from 0.966 to 0.085.
                \item Universal Latent-Space Attack: Instead of altering images directly, this method modifies low-level attributes of the generative model, resulting in an AUC drop from 0.99 to 0.17.
            \end{itemize} 
            \item Black-Box Transfer Attack: This approach uses adversarial examples from a surrogate model to impair a more robust classifier's performance. By transferring adversarial examples they develop their own forensic classifier and trasfer the attack to the target classifier \cite{Wang_2020_CVPR}, the AUC is reduced from 0.96 to 0.22. 
        \end{itemize}
        
        Insights into attack transferability \cite{mardeenpaper2} reveal that attacks effective on one model often struggle against others. Transferability is notably successful within the same family of detectors, such as CNN to CNN or CLIP to CLIP, but less so between different families (e.g., CNN and CLIP). While both CNN and CLIP models are vulnerable to white-box attacks, CLIP models demonstrate greater robustness, particularly against fake-to-real attacks.
        The low transferability of adversarial attacks suggests that distinct model architectures process images differently:
        CNN-based detectors focus on medium-to-high frequencies and isotropic spectra, while CLIP-based detectors rely on low-frequency patterns and cross-shaped spectra.
        This architectural divergence contributes to the limited effectiveness of attacks across model types, indicating that successful defenses must consider these fundamental differences in image processing.
        
        The forger holds a strategic advantage, needing to devise only one successful attack, while the defender must guard against all potential threats. Notably, detectors trained on ImageNet \cite{denglarge} are particularly vulnerable; forensic classifiers require perturbations approximately ten times smaller than those needed to deceive ImageNet classifiers, possibly due to JPEG artifacts present in the training data \cite{mardeenpaper1}.
        Two effective defenses have emerged:
        \begin{itemize}
            \item Adversarial Training: This technique involves continuously training the classifier on adversarial examples generated from previous iterations, enhancing its robustness.
            \item Randomized Smoothing: This method adds significant Gaussian noise to each pixel, making it provably impossible for small perturbations to alter the classifier's output.
        \end{itemize}
        
        Forensic classifiers must integrate an adversarial model into their defenses that extends beyond standard techniques like recompression, resizing, blurring, or adding white noise. This comprehensive approach is essential for improving resilience against increasingly sophisticated attacks.

\section{Experiment}
    In this first phase of the project, our team executed a preliminary experiment, to asses the capability of the laundering attack, from the section \ref{sec:laundering}, on the dataset \textit{TrueFake} provided by the \textit{MMLAB} team.

    The first phase of the experiment consisted in recovering 25 real images and, given their large size, extract 4 patches of size $1024\times1024$ from each of them, for a total of 100 real patches. Next, such patches were laundered, with a \textit{denoising} parameter of 0, using the model \textit{sd\_xl\_base\_1.0} \cite{sdxl_base_1_0}. Lastly, a total of 100 fully synthetic images, generated by \textit{Stable Diffusion XL}, were collected from the \textit{TrueFake} dataset in equal quantity from each category available.

    This small dataset was submitted to the 2 step pipeline from \cite{mandelli2024synthetic}, visualized in Fig. \ref{fig:first_stage}, obtaining interesting results. The first step, as can be seen in Fig. \ref{fig:first_stage},  yielded similar results as \cite{mandelli2024synthetic}, with a good separability around threshold 0. On the other hand, the second step, in Fig. \ref{fig:second_stage}, yielded results slightly different from \cite{mandelli2024synthetic}, in particular the fully synthetic images had a average score of about 1, where \cite{mandelli2024synthetic} reported good separability at threshold 0. 

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.95\linewidth]{Img/first_stage.png}
        \caption{Results of the first step of the pipeline from \cite{mandelli2024synthetic} using images from \textit{MMLAB TrueFake} dataset}
        \label{fig:first_stage}
    \end{figure}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.95\linewidth]{Img/second_stage.png}
        \caption{Results of the second step of the pipeline from \cite{mandelli2024synthetic} using images from \textit{MMLAB TrueFake} dataset}
        \label{fig:second_stage}
    \end{figure}

\section{Conclusions}

\bibliographystyle{IEEEtran} % Stile delle referenze (es. IEEE)
\bibliography{references}    % Nome del file .bib (senza estensione)

\end{document}