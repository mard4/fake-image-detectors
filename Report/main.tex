\documentclass[conference]{IEEEtran} % Usa la classe IEEEtran per formattazione simile

\usepackage{amsmath}
\usepackage{graphicx}



\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

\title{Exploring the Adversarial Robustness of AI-generated Image Detectors}

\author{
    \IEEEauthorblockN{Thomas Lazzerini, Samuele Cappelletti, Martina D'Angelo}
    \IEEEauthorblockA{
        University of Trento
    }
}

\maketitle

\begin{abstract}
    Nowadays, there exists many image generators, so it is important to discriminate between synthetic images and real ones. In this regard, many detectors has been proposed but, at the same time, also many attacks to this detectors has been developed to interfere with their detection capabilities. In this report we present a general overview about some detection techniques and some recently developed attack techniques. 
\end{abstract}

\section{Introduction}
    Synthetic images are now flooding the real world, from online dating sites to social media, fake profiles and scams are everywhere. While some images are funny and harmless, others could be exploited by malicious users \cite{carlini2020evading}. In relation to this, in the image forensic field there is a continuous fight between \textit{image detectors} and \textit{adversarial attacks}. On one hand, the detectors try to discriminate synthetic images from real ones, while, on the other hand, the attacks try to trick the detectors by manipulating the images, both real and synthetic. In order to detect synthetic images, we have two main types of techniques: the \textit{high-level forensic techniques} and the \textit{low-level forensic techniques}. The former focuses on physical inconsistencies \cite{farid2022lighting}\cite{farid2022perspective} and on repeated and uniform patterns \cite{mundra2023exposing}, both of which are mostly visible to the human eye. While, the latter focuses on pixel-level artifacts, almost invisible to the human eye. Since image generators are becoming smarter every day and are able to avoid generating high-level artifacts, we prefer to rely on the low-level ones.

\section{Detectors}
    In this section we provide an overview about some detection techniques. In particular, a smart use of \textit{CLIP} to extract the feature vectors from the images \cite{cozzolino2024raising} and some considerations about the generalization of \textit{GAN} detectors over \textit{Diffusion Models} \cite{corvi2023detection}.

    \subsection{CLIP-Based Detector}

        Many SoTA detectors perform well at detecting images generated by generators of the same family as the ones used during training. However, their performance drops significantly when dealing with images produced by different types of generators. For example, a detector trained on images from GAN-based generators is good with other GANs but struggles with images generated by diffusion-based models.
        
        On the other hand, the CLIP-based detector proposed by \cite{cozzolino2024raising} works well in detecting images generated by any type of generator, both with and without augmentations (e.g., cropping, resizing, compression, etc.). Thanks to the fact that the CLIP features achieve an excellent generalization and robustness even with a few examples (e.g., 1k or 10k), the CLIP-based has a significant performance improvement in the out-of-distribution scenario.

        The CLIP-based method consists in collecting $N$ real images with their corresponding captions and using these to generate $N$ synthetic images. Then, CLIP is used to extract feature vectors from the $N + N$ images and feed them to a linear SVM classifier to perform the classification.
        
        The evaluation of this CLIP-based method, reported in \cite{cozzolino2024raising}, suggests good generalization properties over different generator families (\textit{GAN}, \textit{Diffusion} and \textit{Commercial Tools}), both in the the presence of post-processed images and not.

    \subsection{Detection of Images by Diffusion Models}
        Lately, \textit{Diffusion Models} gained the spotlight in the image generation community, allowing for unmatched test-to-image photorealism and diversity. Most SoTA detectors exploits low-level artifacts, introduced during the generation phase by GAN generators. The study in \cite{corvi2023detection} suggests that similar traces can be found also in DM-generated images.

        The study in \cite{corvi2023detection} also provides interesting evaluation results, comparing the performances of several SoTA detectors over different GAN and DM generators both in ideal case (uncompressed images) and real case (compressed and resized using the guidelines in \cite{vipcuplink}). These evaluations highlight how performances vary significantly between the models, due to the differences in their artifacts, therefore suggesting generalization difficulties (e.g., in classifying DM images with GAN training and vice versa). Despite these difficulties, the inclusion of DM generated images during training and the application of a calibration procedure, like the one suggested by \cite{Platt1999probabilistic}, may help the generalization over similar architectures, despite not providing reliable results on out-of-training artifacts.

\section{Attacks}
    Despite the powerful detectors at our disposal, there exists many users that aim at attacking such detectors, in order to hide traces of their forgeries or also to introduce traces typical of generated images, to disguise real content as synthetic. In this section we provide an overview of some newly developed attacking techniques.

    \subsection{White Black}
        Evaluating image detectors reveals their vulnerabilities to both white-box and black-box attacks, significantly reducing their effectiveness under various attach scenarios. SoTA classifiers are able to achieve almost perfect results in terms of AUC when trained on a single generator \cite{Wang_2020_CVPR}, yet remains susceptible to adversarial perturbations \cite{de2024exploring}. The attacks can be categorized into \textit{white-box attacks}, where full access to the classifier's parameters is available, and \textit{black-box attacks}, where only the classifier type is known.
        
        The work in \cite{de2024exploring} introduced several attack types targeting the classifier in \cite{Wang_2020_CVPR}. Among these, we can find \textit{image-specific attacks}, which modify input images with perturbations, and \textit{universal attacks}, which create a single adversarial perturbation applicable across various images. An interesting example regards the \textit{black-box transfer attack}, which uses adversarial examples from a surrogate model to impair the performance of a more robust classifier.

        The evaluation provided in \cite{de2024exploring} shows that these attack strategies are highly effective and can significantly reduce the AUC performance of the detector.
        
        Insights into attack transferability \cite{de2024exploring} reveal that attacks effective on one model often struggle against others. Transferability is notably successful within the same family of detectors, such as CNN to CNN or CLIP to CLIP, but less so between different families (e.g., CNN and CLIP). While both CNN and CLIP models are vulnerable to white-box attacks, CLIP models demonstrate greater robustness, particularly against fake-to-real attacks.
        The low transferability of adversarial attacks suggests that distinct model architectures process images differently:
        CNN-based detectors focus on medium-to-high frequencies and isotropic spectra, while CLIP-based detectors rely on low-frequency patterns and cross-shaped spectra.
        This architectural divergence contributes to the limited effectiveness of attacks across model types, indicating that successful defenses must consider these fundamental differences in image processing.
        
        The forger holds a strategic advantage, needing to devise only one successful attack, while the defender must guard against all potential threats. Notably, detectors trained on ImageNet \cite{denglarge} are particularly vulnerable; forensic classifiers require perturbations approximately ten times smaller than those needed to deceive ImageNet classifiers, possibly due to JPEG artifacts present in the training data \cite{de2024exploring}.
        Two effective defenses have emerged:
        \begin{itemize}
            \item Adversarial Training: This technique involves continuously training the classifier on adversarial examples generated from previous iterations, enhancing its robustness.
            \item Randomized Smoothing: This method adds significant Gaussian noise to each pixel, making it provably impossible for small perturbations to alter the classifier's output.
        \end{itemize}
        
        Forensic classifiers must integrate an adversarial model into their defenses that extends beyond standard techniques like recompression, resizing, blurring, or adding white noise. This comprehensive approach is essential for improving resilience against increasingly sophisticated attacks.

    \subsection{Mimicry attack against image splicing forensic}
        As stated in \cite{boato2024adversarial}, this \textit{mimicry adversarial attack} can be used to hide image manipulation while forcing the detector to detect arbitrary ones by applying a gradient based optimization approach. Applied at large scale, this would cause high false-alarms, producing an effect similar to \textit{DoS} attacks while undermining the reliability of the target detector.

        The attack strategy proposed in \cite{boato2024adversarial} involves splitting the image in uniform patches and use these to compute a target representation for both the \textit{pristine patch $t_p$}, computed from the pristine patches, and the \textit{forged patch $t_f$}, computed from the forged patches. The function used for computing such target representations needs to be defined for each detector for the attack to be effective, this due to the fact that different detectors exploit different features. Once the targets have been computed, a gradient-based iterative approach is applied the each patch of the manipulated image, in order to make the patch feature representation more similar to the respective target's feature representation. A visualization of such iterative approach can be seen in Fig. \ref{fig:mim_visual}

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\linewidth]{Img/mim_visual.png}
            \caption{Visualization of the mimicry attack strategy and its effects proposed in \cite{boato2024adversarial}. The "\textit{ground truth}" tampering map represents the real forgery, while the "\textit{target}" tampering map represent the arbitrary forgery the attacker wants the detector to output}
            \label{fig:mim_visual}
        \end{figure}

        The evaluation results reported in \cite{boato2024adversarial} suggests this attack is highly effective, both in hiding the real forgery and also highlighting a "decoy" forgery arbitrarily introduced. Two image detectors were tested, \textit{Noiseprint} \cite{cozzolino2019noiseprint} and \textit{EXIF-SC} \cite{huh2018fighting}, over two different datasets, \textit{Columbia} \cite{ng2004data} and \textit{DSO-1} \cite{carvalho2015illuminant}. Several threshold-based and threshold-less metrics have been tested, with the latter being more important from the attacker point of view since the threshold values are unknown to him.

        Another interesting result presented in \cite{boato2024adversarial} regards the \textit{cross-detector} scenario, in which the attack is performed targeting a specific detector but then another is used in the evaluation. Also, \textit{stacked attacks} are considered, in which an image is sequentially attacked against different detectors. An evaluation in these regards reveal mixed results: a misaligned attack in not effective, while the performances of a stacked attack are highly dependant on both the order of the attacks and the detector used in the evaluation. Nevertheless, this is an interesting scenario open for further studies.

    \subsection{Image Laundering with Stable Diffusion}
        \label{sec:laundering}
        Differently from "classic" diffusion models, like \textit{Latent Diffusion}, \textit{Stable Diffusion} models allow the users to provide an initial image as input \cite{sd1_github} \cite{sd2_github} \cite{podell2023sdxl} \cite{sauer2023adversarial-sdxl-turbo}. This image will be superimposed with noise and modified by the model according to the textual prompt. The weight of such modifications can be set via a dedicated strength parameter in the range $[0, 1]$.

        Processing images in such pipeline using a strength parameter equal to 0 produces outputs with the maximum similarity to the inputs: the image is encoded and decoded right away, without any denoising step. As suggested in \cite{mandelli2024synthetic}, this process could be exploited by malicious users in order to mask real content as synthetic. In fact, the encoding/decoding is sufficient to introduce enough artifacts into the real image to make it synthetic in the eyes of numerous detectors. This practice is known as \textit{image laundering}.

        The study in \cite{mandelli2024synthetic}, proposes a two-step architecture, visualized in Fig. \ref{fig:launder_2_stage}, as solution to efficiently discriminate between real, fully synthetic and laundered images. Such architecture is inspired by \cite{mandelli2022detecting}, in which the image is split into multiple random patches, a score is assigned to each one and the average o the highest scores provide the global score of the image: a positive score suggest a synthetic image, while a negative score a real one. Despite the good results, this backbone architecture alone is unfit for the laundered image detection task, hence the introduction of the 2 steps: the first step discriminate real from synthetic images, while the second step discriminate fully synthetic from laundered images.

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.95\linewidth]{Img/launder_2_stage.png}
            \caption{Visualization of the 2 step architecture proposed in \cite{mandelli2024synthetic} for the laundered image classification task. The first step discriminate real images from synthetic (both fully and laundered) ones, while the second step discriminate fully synthetic images from laundered ones}
            \label{fig:launder_2_stage}
        \end{figure}

        The evaluation provided in \cite{mandelli2022detecting} about such 2 step pipeline are good: the first step reach a good separability between real and synthetic image, while the second step reach almost perfect results over multiple models and multiple metrics, with only minor decreases in performances when post-processing operations like JPEG compression and resize are applied.

\section{Experiment}
    In this first phase of the project, our team executed a preliminary experiment, to asses the capability of the laundering attack, from the section \ref{sec:laundering}, on the dataset \textit{TrueFake} provided by the \textit{MMLAB} team.

    The first phase of the experiment consisted in recovering 25 real images and, given their large size, extract 4 patches of size $1024\times1024$ from each of them, for a total of 100 real patches. Next, such patches were laundered, with a \textit{denoising} parameter of 0, using the model \textit{sd\_xl\_base\_1.0} \cite{sdxl_base_1_0}. Lastly, a total of 100 fully synthetic images, generated by \textit{Stable Diffusion XL}, were collected from the \textit{TrueFake} dataset in equal quantity from each category available.

    This small dataset was submitted to the 2 step pipeline from \cite{mandelli2024synthetic}, visualized in Fig. \ref{fig:first_stage}, obtaining interesting results. The first step, as can be seen in Fig. \ref{fig:first_stage},  yielded similar results as \cite{mandelli2024synthetic}, with a good separability around threshold 0. On the other hand, the second step, in Fig. \ref{fig:second_stage}, yielded results slightly different from \cite{mandelli2024synthetic}, in particular the fully synthetic images had a average score of about 1, where \cite{mandelli2024synthetic} reported good separability at threshold 0. 

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.95\linewidth]{Img/first_stage.png}
        \caption{Results of the first step of the pipeline from \cite{mandelli2024synthetic} using images from \textit{MMLAB TrueFake} dataset}
        \label{fig:first_stage}
    \end{figure}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.95\linewidth]{Img/second_stage.png}
        \caption{Results of the second step of the pipeline from \cite{mandelli2024synthetic} using images from \textit{MMLAB TrueFake} dataset}
        \label{fig:second_stage}
    \end{figure}

\section{Conclusions}

\bibliographystyle{IEEEtran} % Stile delle referenze (es. IEEE)
\bibliography{references}    % Nome del file .bib (senza estensione)

\end{document}