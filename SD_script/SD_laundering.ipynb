{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export by strenghts\n",
    "\n",
    "## Strenghts values to try out:\n",
    "0, 0.05, 0.1, 0.15 e 0.2 di strength\n",
    "\n",
    "## Output\n",
    "folder: dataset_str_00\n",
    "imgs: file name stays the same\n",
    "\n",
    "- Try also SD2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to /media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/input_imgs/\n"
     ]
    }
   ],
   "source": [
    "# unzip image folder\n",
    "zip_file_path = \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/input_imgs/FFHQ.zip\"\n",
    "import zipfile\n",
    "import os\n",
    "destination_dir = \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/input_imgs/\"\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(destination_dir)\n",
    "print(f\"Files extracted to {destination_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "# Load the img2img pipeline\n",
    "pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guida ai Parametri di Stable Diffusion XL\n",
    "\n",
    "---\n",
    "\n",
    "## **`num_inference_steps`**\n",
    "**Descrizione**: Numero di iterazioni che il modello esegue per generare l'immagine.  \n",
    "**Intervallo di valori comuni**: 20–100.  \n",
    "\n",
    "### Effetti:\n",
    "- **Valori bassi** (es. 20–30): più veloci ma con meno dettagli e stabilità.\n",
    "- **Valori alti** (es. 50–100): immagini più dettagliate e stabili, ma più lente da calcolare.\n",
    "\n",
    "---\n",
    "\n",
    "## **`guidance_scale`**\n",
    "**Descrizione**: Quanto il modello aderisce al prompt rispetto alla sua creatività.  \n",
    "**Intervallo di valori comuni**: 5.0–15.0.  \n",
    "\n",
    "### Effetti:\n",
    "- **Valori bassi** (5.0–7.0): generazioni più creative e meno vincolate al prompt.\n",
    "- **Valori alti** (10.0–15.0): generazioni più aderenti al prompt, ma meno creative.\n",
    "\n",
    "### Suggerimenti:\n",
    "- Usa valori **alti** per risultati precisi (es. 12.0).\n",
    "- Usa valori **bassi** per esplorare variazioni creative.\n",
    "\n",
    "---\n",
    "\n",
    "## **`strength`**\n",
    "**Descrizione**: Controlla quanto l'immagine iniziale influenza il risultato finale.  \n",
    "**Intervallo di valori comuni**: 0.1–1.0.  \n",
    "\n",
    "### Effetti:\n",
    "- **Valori bassi** (0.1–0.4): preservano molti dettagli dell'immagine iniziale.\n",
    "- **Valori alti** (0.7–1.0): trasformano maggiormente l'immagine iniziale secondo il prompt.\n",
    "\n",
    "### Suggerimenti:\n",
    "- Usa valori **bassi** per modifiche leggere.\n",
    "- Usa valori **alti** per cambiamenti radicali.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.41it/s]:00<?, ?file/s]\n",
      "Processing Images:   0%|          | 1/500 [00:02<18:15,  2.20s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved at: dataset_str_0.1/35410.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.88it/s]\n",
      "Processing Images:   0%|          | 2/500 [00:04<16:41,  2.01s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved at: dataset_str_0.1/28728.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Generate the image\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                          \u001b[49m\u001b[38;5;66;43;03m# Input image\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# Degree of transformation\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Creativity\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Save the generated image with the same name\u001b[39;00m\n\u001b[1;32m     68\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, image_file)  \u001b[38;5;66;03m# Modified: Preserve original name\u001b[39;00m\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py:1272\u001b[0m, in \u001b[0;36mStableDiffusionXLImg2ImgPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, image, strength, num_inference_steps, timesteps, sigmas, denoising_start, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, aesthetic_score, negative_aesthetic_score, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1250\u001b[0m (\n\u001b[1;32m   1251\u001b[0m     prompt_embeds,\n\u001b[1;32m   1252\u001b[0m     negative_prompt_embeds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     clip_skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_skip,\n\u001b[1;32m   1269\u001b[0m )\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;66;03m# 4. Preprocess image\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;66;03m# 5. Prepare timesteps\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoising_value_valid\u001b[39m(dnv):\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/diffusers/image_processor.py:674\u001b[0m, in \u001b[0;36mVaeImageProcessor.preprocess\u001b[0;34m(self, image, height, width, resize_mode, crops_coords)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdo_convert_grayscale:\n\u001b[1;32m    673\u001b[0m         image \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_grayscale(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m image]\n\u001b[0;32m--> 674\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpil_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to np\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_to_pt(image)  \u001b[38;5;66;03m# to pt\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/diffusers/image_processor.py:172\u001b[0m, in \u001b[0;36mVaeImageProcessor.pil_to_numpy\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m    170\u001b[0m     images \u001b[38;5;241m=\u001b[39m [images]\n\u001b[1;32m    171\u001b[0m images \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(image)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 172\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/numpy/_core/shape_base.py:464\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    462\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[1;32m    463\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set a debug limit (change this value to None for no limit)\n",
    "DEBUG = None  # Limit for debugging; set to None to process all images\n",
    "\n",
    "# Input folder containing all images\n",
    "input_folder = \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/input_imgs/FFHQ\"\n",
    "\n",
    "\n",
    "# Prompt and negative prompt\n",
    "prompt = (\n",
    "    \"\"\n",
    ")\n",
    "negative = (\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "# Parameters\n",
    "num_inference_steps = 50  # Passi di inferenza medi, bilanciando velocità e dettaglio.\n",
    "guidance_scale = 10  # Livello intermedio-alto per una buona creatività mantenendo una chiara aderenza al prompt.\n",
    "strength = 0.1  # Degree of transformation (0.0 = no change, 1.0 = full transformation)\n",
    "height = 1024  # Image height (adjust as needed)\n",
    "width = 1024   # Image width (adjust as needed)\n",
    "\n",
    "# Output folder\n",
    "output_folder = f\"dataset_str_{str(strength)}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Iterate through the input folder\n",
    "image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "if DEBUG:\n",
    "    image_files = image_files[:DEBUG]  # Apply debug limit here\n",
    "\n",
    "for idx, image_file in enumerate(tqdm(image_files, desc=\"Processing Images\", unit=\"file\", position=0)):\n",
    "    input_image_path = os.path.join(input_folder, image_file)\n",
    "\n",
    "    # Load and preprocess the input image\n",
    "    if not os.path.exists(input_image_path):\n",
    "        print(f\"Skipping missing file: {input_image_path}\")\n",
    "        continue\n",
    "\n",
    "    init_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "    init_image = init_image.resize((width, height))  # Resize to model dimensions\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    #seed = random.randint(1, 1000)\n",
    "    #print(f\"Processing '{image_file}' with Seed: {seed}\")\n",
    "    seed=42\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "    # Generate the image\n",
    "    image = pipeline(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative,\n",
    "        image=init_image,                          # Input image\n",
    "        strength=strength,                         # Degree of transformation\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,             # Creativity\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    # Save the generated image with the same name\n",
    "    output_path = os.path.join(output_folder, image_file)  # Modified: Preserve original name\n",
    "    #image.save(output_path)\n",
    "\n",
    "    print(f\"Generated image saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders zipped into /media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/FFHQ_strength_0.1_0.05_0.15.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the folders to zip and the destination zip file\n",
    "folders_to_zip = (\n",
    "    \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/dataset_str_0.1\",\n",
    "    #\"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/dataset_str_0.2\",\n",
    "    \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/dataset_str_0.05\",\n",
    "    \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/dataset_str_0.15\"\n",
    ")\n",
    "destination_dir = \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/\"\n",
    "zip_file_path = os.path.join(destination_dir, \"FFHQ_strength_0.1_0.05_0.15.zip\")\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Create a zip file and add the folders\n",
    "with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for folder in folders_to_zip:\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Add file to zip, keeping folder structure\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(folders_to_zip[0]))\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"Folders zipped into {zip_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /media/SSD_mmlab/martina.dangelo/fake-image-detectors/dataset/media/00000.png\n"
     ]
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# path = \"/media/NAS/TrueFake/PreSocial/Fake/StableDiffusionXL/faces\"\n",
    "# save_path = \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/dataset/media\"  # Replace with your desired save directory\n",
    "\n",
    "# if not os.path.exists(save_path):\n",
    "#     os.makedirs(save_path)  # Create the directory if it doesn't exist\n",
    "\n",
    "# for i in os.listdir(path):\n",
    "#     input_image_path = os.path.join(path, i)\n",
    "#     output_image_path = os.path.join(save_path, i)\n",
    "\n",
    "#     try:\n",
    "#         with Image.open(input_image_path) as img:\n",
    "#             img.save(output_image_path)  # Save the image in the save_path directory\n",
    "#         print(f\"Saved: {output_image_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file {i}: {e}\")\n",
    "#     break  # Remove this break if you want to process all files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################ ###############################\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()  # Clears the cache (useful for preventing memory fragmentation)\n",
    "torch.cuda.memory_allocated()  # Double-check if memory is now free\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
