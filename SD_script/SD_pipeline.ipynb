{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map\n",
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Carica il modello dalla piattaforma Hugging Face\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "# Configura la pipeline\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    use_safetensors=True,  # Per performance migliori\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mc donalds's Nugget on a snowboard with a cowboy hat, in front of a snowy mountainous landscape with clear weather.\"\n",
    "negative = \"blurry, low quality, pixelated, out of focus, overexposed, underexposed, poorly detailed, text artifacts, watermark, grainy textures, flat colors, monochrome, color bleeding, low resolution, bad composition.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed:  307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "num_inference_steps=50\n",
    "guidance_scale=7.5\n",
    "\n",
    "height = 1024  # Altezza in pixel\n",
    "width = 1024  # Larghezza in pixel\n",
    "\n",
    "# random seed\n",
    "seed = random.randint(1,1000)\n",
    "print(\"Using Seed: \",seed)\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "\n",
    "# Generazione dell'immagine\n",
    "\n",
    "### strenght  \n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative,                   # Prompt negativo\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_inference_steps=num_inference_steps,    # Passi di inferenza (maggiore = più dettagli)\n",
    "    guidance_scale= guidance_scale,              # Peso per il prompt (maggiore = risultati più aderenti)\n",
    "    generator=generator\n",
    ").images[0]\n",
    "\n",
    "# Salva l'immagine generata\n",
    "image.save(f\"imgs/{str(seed)}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMG TO IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set parameters\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  2.65it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed:  431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmanual_seed(seed)  \u001b[38;5;66;03m# Use \"cuda\" for the generator if running on GPU\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Generate the image\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                          \u001b[49m\u001b[38;5;66;43;03m# Input image\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# Degree of transformation\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Save the generated image\u001b[39;00m\n\u001b[1;32m     48\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:1088\u001b[0m, in \u001b[0;36mStableDiffusionImg2ImgPipeline.__call__\u001b[0;34m(self, prompt, image, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1085\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1152\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         emb \u001b[38;5;241m=\u001b[39m emb \u001b[38;5;241m+\u001b[39m class_emb\n\u001b[0;32m-> 1152\u001b[0m aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aug_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maddition_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_hint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1156\u001b[0m     aug_emb, hint \u001b[38;5;241m=\u001b[39m aug_emb\n",
      "File \u001b[0;32m/media/SSD_mmlab/martina.dangelo/fake-image-detectors/myvenv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:970\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_aug_embed\u001b[0;34m(self, emb, encoder_hidden_states, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m     aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_embedding(text_embs, image_embs)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maddition_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# SDXL - style\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_embeds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m:\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    972\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has the config param `addition_embed_type` set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_time\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which requires the keyword argument `text_embeds` to be passed in `added_cond_kwargs`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m     text_embeds \u001b[38;5;241m=\u001b[39m added_cond_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "input_image_path = \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/input_imgs/nug.jpg\"  # Path to your input image\n",
    "output_folder = \"imgs\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "prompt = \"A futuristic cityscape illuminated by neon lights\"\n",
    "negative = \"blurry, low quality, pixelated, cartoonish, text artifacts\"\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "strength = 0.75  # Degree of transformation (0.0 = no change, 1.0 = full change)\n",
    "height = 1024  # Image height\n",
    "width = 1024   # Image width\n",
    "\n",
    "# Load the pipeline\n",
    "pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Load and preprocess the input image\n",
    "if not os.path.exists(input_image_path):\n",
    "    raise FileNotFoundError(f\"Input image '{input_image_path}' not found.\")\n",
    "    \n",
    "init_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "init_image = init_image.resize((width, height))  # Resize to model dimensions\n",
    "\n",
    "# Validate prompt\n",
    "if not prompt:\n",
    "    raise ValueError(\"Prompt cannot be empty.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = random.randint(1, 1000)\n",
    "print(\"Using Seed: \", seed)\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)  # Use \"cuda\" for the generator if running on GPU\n",
    "\n",
    "# Generate the image\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative,\n",
    "    image=init_image,                          # Input image\n",
    "    strength=strength,                         # Degree of transformation\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "# Save the generated image\n",
    "output_path = os.path.join(output_folder, f\"{str(seed)}.png\")\n",
    "image.save(output_path)\n",
    "\n",
    "print(f\"Generated image saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# img to img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from diffusers import StableDiffusionImg2ImgPipeline\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import os\n",
    "\n",
    "# # Parameters\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"  # Compatible model for img2img\n",
    "# output_folder = \"imgs\"\n",
    "# input_image_path = \"/media/SSD_mmlab/martina.dangelo/fake-image-detectors/SD_script/input_imgs/nug.jpg\"  # Path to your input image\n",
    "\n",
    "# os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "# prompt = (\n",
    "#     \"A highly detailed and realistic depiction of a golden, crispy chicken nugget on a snowboard, wearing a classic cowboy hat. \"\n",
    "#     \"The nugget is situated in a snowy mountainous landscape under clear blue skies. \"\n",
    "#     \"The composition is photorealistic and humorous, with the nugget blending seamlessly into the environment.\"\n",
    "# )\n",
    "# negative = (\n",
    "#     \"blurry, low quality, pixelated, out of focus, poorly detailed, cartoonish, \"\n",
    "#     \"text artifacts, watermarks, deformed features, oversaturated colors, \"\n",
    "#     \"grainy, flat shading, exaggerated proportions, unrealistic textures, bad composition, noise.\"\n",
    "# )\n",
    "\n",
    "\n",
    "# num_inference_steps = 50\n",
    "# guidance_scale = 7.5\n",
    "# strength = 0.75  # Degree of transformation (0.0 = no change, 1.0 = full transformation)\n",
    "# height = 512  # Image height (adjust as needed)\n",
    "# width = 512   # Image width (adjust as needed)\n",
    "\n",
    "# # Load the img2img pipeline\n",
    "# pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float16,\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# # Load and preprocess the input image\n",
    "# if not os.path.exists(input_image_path):\n",
    "#     raise FileNotFoundError(f\"Input image '{input_image_path}' not found.\")\n",
    "\n",
    "# init_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "# init_image = init_image.resize((width, height))  # Resize to model dimensions\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# seed = random.randint(1, 1000)\n",
    "# print(\"Using Seed:\", seed)\n",
    "# generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "# # Generate the image\n",
    "# image = pipeline(\n",
    "#     prompt=prompt,\n",
    "#     negative_prompt=negative,\n",
    "#     image=init_image,                          # Input image\n",
    "#     strength=strength,                         # Degree of transformation\n",
    "#     num_inference_steps=num_inference_steps,\n",
    "#     guidance_scale=guidance_scale,\n",
    "#     generator=generator,\n",
    "# ).images[0]\n",
    "\n",
    "# # Save the generated image\n",
    "# output_path = os.path.join(output_folder, f\"{str(seed)}.png\")\n",
    "# image.save(output_path)\n",
    "\n",
    "# print(f\"Generated image saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  4\n",
      "1/16\n",
      "2/16\n",
      "3/16\n",
      "4/16\n",
      "5/16\n",
      "6/16\n",
      "7/16\n",
      "8/16\n",
      "9/16\n",
      "10/16\n",
      "11/16\n",
      "12/16\n",
      "13/16\n",
      "14/16\n",
      "15/16\n",
      "16/16\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "PATCH_SIZE = 1024\n",
    "DEBUG = False\n",
    "WRITE = True\n",
    "FOLDER_PATH = \"imgs/\"\n",
    "OUTPUT_FOLDER_PATH = \"cropped_imgs/\"\n",
    "FILE_FORMAT = (\".png\", \".jpg\")\n",
    "\n",
    "\n",
    "# Recupera lista img da FOLDER_PATH\n",
    "images = [os.path.join(FOLDER_PATH, file) for file in os.listdir(FOLDER_PATH) if file.lower().endswith(FILE_FORMAT)]\n",
    "\n",
    "print(\"Total images: \",len(images))\n",
    "os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)  # Crea la directory se non esiste\n",
    "\n",
    "PROCESSED_IMG = len(images) \n",
    "PATCH_FOR_IMG = 4\n",
    "\n",
    "# Process x images\n",
    "random_image_list = random.sample(images, PROCESSED_IMG)\n",
    "\n",
    "counter_tot = PROCESSED_IMG * PATCH_FOR_IMG\n",
    "current_counter = 1\n",
    "for file_name in random_image_list:\n",
    "    img = cv2.imread(file_name)\n",
    "\n",
    "    max_y = img.shape[0]-PATCH_SIZE\n",
    "    max_x = img.shape[1]-PATCH_SIZE\n",
    "\n",
    "    file_name_no_ext, ext = os.path.splitext(os.path.basename(file_name))\n",
    "\n",
    "    # Crop x patches\n",
    "    for patch_number in range(0, PATCH_FOR_IMG):\n",
    "        top_left = (random.randint(0, max_x), random.randint(0, max_y))\n",
    "        bot_right = (top_left[0]+PATCH_SIZE, top_left[1]+PATCH_SIZE)        \n",
    "\n",
    "        cropped_image = img[top_left[1]:top_left[1]+PATCH_SIZE, top_left[0]:top_left[0]+1024]\n",
    "\n",
    "        if(DEBUG):\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            cv2.rectangle(img_rgb, top_left, bot_right, (255, 0, 0), 20)  # Drawing in RGB\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(img_rgb)\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(cropped_image_rgb)\n",
    "            plt.title(\"Cropped Patch\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "        save_name = OUTPUT_FOLDER_PATH + \"/\" + file_name_no_ext + \"-\" + str(patch_number+1) + \".png\"\n",
    "\n",
    "        if WRITE:\n",
    "            cv2.imwrite(str(save_name), cropped_image)\n",
    "\n",
    "        print(str(current_counter) + \"/\" + str(counter_tot))\n",
    "        current_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
